\chapter{Big Data and NoSQL}

From their creation in the 80s up until the 2000s, relational DBMSs were the main type of systems used to manage data. But during the mid 2000s, the rise in popularity of Internet caused a rapid increase of amount of data generated by Internet activity. A new set of application fields, called \textbf{Big Data}, were born, and the capabilities of traditional relational DBMSs were not sufficient. Specifically, the limitations of relational databases are represented by the ``three Vs'': \textbf{volume}, because these applications deal with massive amount of data, and DBMSs do not scale enough; \textbf{velocity}, referring both to computational speed, and to development speed; \textbf{variety}, since data is now highly heterogeneous, and schemas often conflict with variety.

NoSQL refers to the broad group of non-relational database systems. Some of them may have query languages, but most of them do not implement actual SQL. For some applications ACID is given up, in favor of greater support for distribution. Also, since data is no longer represented by tables, they cannot be represented in first normal form; this can greatly reduce the need for joins, but at a cost (explained later). Since they are free from some constraints of SQL-based relational databases, NoSQL systems have become the preferred choice for Big Data applications.

Another big reason why NoSQL systems are favored is the \textbf{impedance mismatch}, i.e., the difference between the relational model and the in-memory data structures. In a relational data model, information is stored in name-value pairs (tuples), which in turn are organized in sets (relations). This representation is simple and effective, but tuples have to be simple: they cannot contain any data structure, such as lists, nested records, trees, and so on. But in-memory, that same data is in fact represented using richer data structures, and this requires a translation to the relational representation. Instead, NoSQL are not forced to use tuples and relations, so the impedance mismatch is not a problem.

When SQL was first developed, the role of a database was that of an \textbf{integration database}, storing data shared by multiple separate application, each managed by a different team. A different approach is to treat the database as an \textbf{application database}, only accessed by a single application looked after by a single team. Since the application team also controls the database, the responsibility for database integrity can be moved in the application code.

Finally, many organizations started using cluster architectures (buying multiple cheap machines that work together instead of using a single big machine), and NoSQL systems were designed from the beginning to run on such architectures (i.e., Google BigTable, Amazon Dynamo).

Note that NoSQL is different from NewSQL (which includes column databases, in-memory databases, etc.).

\section{Consistency}

These systems do not provide traditional transactions, but must still guarantee some level of consistency. The three problems to consider are write-write conflicts, read consistency (how fresh is the data, if there is intermediate data, session consistency), and transactional consistency (only write values that are based on currently valid data).

A commonly cited reason for wanting to relax consistency in NoSQL systems is the \textbf{CAP theorem}. It states that given the properties of \textit{Consistency}, \textit{Availability}, and \textit{Partition tolerance}, you can only have two. In database systems, the solution is to usually give up availability (e.g., two phase commit). In NoSQL systems, consistency is relaxed instead.

When consistency is traded off, it's not completely given up, though. The more the nodes are involved in a request, the higher is the chance of avoiding an inconsistency. 

The first issue is that many copies of the same value must be consistently accessed. In P2P systems, not all nodes must acknowledge a write to ensure strong consistency, just the majority: the \textbf{write quorum} can be set to any $w > n/2$. The \textbf{read quorum} is then set to $r > n - w$ to again guarantee strongly consistent reads.

The second issue is updating a value only if no other node changed it in the meanwhile: an optimistic approach uses \textit{version stamps}, meaning that each time a data item is read, it will be associated to a timestamp, updated after the item is updated. The update has the previous version's parameter and fails if the timestamp at the moment of actually writing is different (\textbf{Compare-And-Set}).

If no quorums are used, then \textbf{reconciliation} is used, allowing the system to diverge. Reconciliation is heavily system-dependent: if the only possible operation is an insert to a data structure, reconciliation may be simply adding both elements; if both inserts and updates are possible, the minimal set of insertions and deletes compatible with both versions is found and applied to their common ancestor. Some systems use a human-in-the-loop approach, simply asking a human user to solve inconsistencies. If items are associated with timestamps, reconciliation considers the most recent. In P2P systems, the temporal relationship between different versions has to be decided to choose which way the data will be modified. Basic version IDs are not enough, because they are often local counters, or global random (unordered) unique IDs. One proposed solution is the \textbf{vector clock}, a map from node IDs to local counters of each local counter.

\section{Map-Reduce}

The map-reduce pattern is a way to organize processing to take advantage of splitting computation across multiple machines on a cluster while keeping as much processing and the data it needs on the same machine.

The first step is \textbf{mapping}. A map is a function whose input is a single aggregate (e.g., a document), and whose output is a set of key-value pairs. Each application of the map function is independent from all others, allowing them to be parallelizable. A map operation operates on a single record; the \textbf{reduce} function takes multiple map outputs with the same key and combines their values.

The Hadoop implementation of the algorithm stores the input and output of each phase in a distributed file system that manages partitioning and replication; the Spark implementation tries to keep, when possible, input and output in main memory.

\section{Types of NoSQL Systems}

NoSQL is not a uniform set of systems. They usually don't support SQL, they're usually open source, cluster-oriented, relatively recent (after the year 2000), schema free, oriented towards a single application. It can be seen as more of a movement than an actual technology.

Many NoSQL applications support materialized views. Although they don't have actual views, they may have pre-computed and cached queries, and use the term materialized views to refer to them.

The data models of NoSQL can be divided in two groups: aggregate data models (key-value stores, document databases, column-family stores), and graph models. Aggregate models store information as collections of aggregated objects instead of tables; this representation is essential to allow working without transactions and joins. 

\paragraph{Key-value Stores}

Key-value stores (Dynamo, Riak, Voldemort) store uninterpreted blocks, as $<$key, value$>$ pairs. The only search allowed is retrieving a file giving its key.

The implementation model is trivial: the data is distributed across a huge farm of inexpensive machines, with constant time access and constant time parallel execution on all the pairs. Fault tolerance is flexible: if the aim of analysis is computing simple statistics and approximation is not a big issue, whenever a node fails it can simply be ignored. If instead a precise computation is required, a strict fault tolerance can be implemented.

Data is first partitioned among nodes through \textbf{sharding}, and then \textbf{replicated}. Replication is done by either \textbf{master-slave replication} (a node is set as the master who holds updated copies and is queried for them, and different levels of consistency can be chosen, e.g., specifying a quorum), or \textbf{P2P replication}. These systems allow the data to become inconsistent, but offer tools to choose how to reconciliate these inconsistencies.

Usually, key-value databases use \textbf{single object consistency}. This means that there is some form of atomicity regarding operations performed on one node. In Riak, each bucket (i.e. dataspace) has its own number of replicas, and a custom read/write quorum can be chosen. If conflicts are allowed, either the newest write wins or a ``sibling'' is created, meaning that the administrator must choose which one to delete.

Queries can be done by key or be full store scans.

These systems are often used to model session information, user profiles, shopping cart data.

\paragraph{Document Databases}

In document databases keys are associated to \textbf{trees}. Some systems use XML (MarkLogic, eXist), others JSON (CouchDB, Membase, Couchbase, MongoDB). Since they store documents, they can be searched by content.

In MongoDB, there is one instance, many databases, and many collections of JSON documents. Each document has its own ID, so MongoDB can query both by key and both in more complex ways. It also uses sharding and replication like key-value databases. Consistency is dealt using master/slave replication of the entire data collection, and the master is dynamically re-elected over a fail. A write quorum can be set. It supports selection, projection, and aggregation, but not joins.

CouchDB supports views (both materialized and virtual).

\paragraph{Column-family Stores}

Column-Family stores (BigTable, HBase, Cassandra) represent data as sets $<$key, record$>$ pairs, grouping columns into ``column families'', similar to tables in relational databases. These families can be divided into keyspaces according to the values of the keys. The records in a family are not necessarily homogeneous.

In Cassandra, for every keyspace the database administrator fixes a number of replicas. The programmer chooses the quorum for read and write operations. Atomicity is guaranteed at the low level. Queries can obtain entire rows, or select field. Indexes can be created on columns. Cassandra also supports CQL (Cassandra Query Language), which implements select and project operations (but not join).

\paragraph{Sparse Table Databases}

Sparse table databases represents data using records, but different records may follow different schemas.

\paragraph{Graph Databases}

Graph databases are rarely used in the field of Big Data since they aren't suitable to store big amounts of data and don't support fast access, however they are very well optimized for variety.

In a graph model, every element is a node with a set of properties, and nodes are connected to each other with labeled edges with a property that describes the type of connection. An edge has the form Edge(NodeId1, NodeId2, EdgeAttributes); there may be also be nodes in the form Node(NodeId, NodeAttribute), to store additional information about nodes. 

Once built, a graph database allows efficient queries that traverse different ``relationships'' between nodes, since it does not require joins. This is because the cost of navigating relationships is largely part of insert time instead of search time: when a node is added, all the connections with other nodes are also established, so a search only has to traverse the graph to find the relevant information.

Graph databases are normally neither sharded nor transactional. Neo4J does support master-slave replication: data can be sharded at the application level with no database support, which is however very difficult. Neo4J implements the query language Cypher, which implements Selects, Projects, both on nodes and edges. Queries can also select paths between nodes, choosing the type of attribute, direction, and number of edges making up that path: this is similar to a join.

These databases are especially useful for modeling social graphs.

\section{Big Data Architectures}

A current trend is \textbf{data lakes}. While in a data warehouse there is a long, complex phase of data cleaning, transforming it to be better suited to whatever operations the data warehouse must support, a data lake receives all the data without any preprocessing. This information is then queried using data mining or AI techniques which ``automatically'' extract insight.

Another trend is that of \textbf{polyglot systems}, that is, systems that gather data from different systems that use different query languages. 